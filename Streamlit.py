import streamlit as st
import googleapiclient.discovery
import pandas as pd
from datetime import datetime, timedelta, timezone
from openai import OpenAI
import re
from collections import Counter
import plotly.express as px

# ========= Streamlit ç¾åŒ– =========
st.set_page_config(page_title="YouTubeé »é“åˆ†æå·¥å…·", page_icon="ğŸ“Š", layout="wide")
st.markdown("""
    <style>
    .main {background-color: #f0f2f6; padding: 20px;}
    h1 {color: #1a73e8;}
    .stButton>button {background-color: #1a73e8; color: white;}
    </style>
""", unsafe_allow_html=True)

# ========= API åˆå§‹åŒ– (å®‰å…¨åœ°å¾ st.secrets è®€å–) =========
try:
    YOUTUBE_API_KEY = st.secrets["YOUTUBE_API_KEY"]
    OPENAI_API_KEY = st.secrets["OPENAI_API_KEY"]
    youtube = googleapiclient.discovery.build("youtube", "v3", developerKey=YOUTUBE_API_KEY)
    client = OpenAI(api_key=OPENAI_API_KEY)
except FileNotFoundError:
    st.error("éŒ¯èª¤ï¼šæ‰¾ä¸åˆ° secrets.toml æª”æ¡ˆã€‚è«‹å…ˆå»ºç«‹ .streamlit/secrets.toml ä¸¦è¨­å®šæ‚¨çš„ API é‡‘é‘°ã€‚")
    st.stop()
except KeyError:
    st.error("éŒ¯èª¤ï¼šè«‹åœ¨ secrets.toml ä¸­è¨­å®š 'YOUTUBE_API_KEY' å’Œ 'OPENAI_API_KEY'ã€‚")
    st.stop()


# ========= åŠŸèƒ½æ¨¡çµ„ (èˆ‡æ‚¨åŸæœ¬çš„ç¨‹å¼ç¢¼ç›¸åŒï¼Œç¨ä½œå„ªåŒ–) =========
@st.cache_data(ttl=3600) # å¿«å–è³‡æ–™ 1 å°æ™‚ï¼Œé¿å…é‡è¤‡æŠ“å–
def get_channel_info(channel_id):
    """æŠ“å–é »é“åŸºæœ¬è³‡è¨Šï¼ŒåŒ…å« uploads_playlist_id å’Œé »é“åç¨±"""
    request = youtube.channels().list(part="contentDetails,snippet", id=channel_id)
    response = request.execute()
    if not response.get("items"):
        return None, None
    item = response["items"][0]
    uploads_playlist_id = item['contentDetails']['relatedPlaylists']['uploads']
    channel_title = item['snippet']['title']
    return uploads_playlist_id, channel_title

@st.cache_data(ttl=3600)
def get_channel_videos(uploads_playlist_id, max_videos=1000):
    # ... (æ­¤å‡½å¼å…§å®¹èˆ‡æ‚¨æä¾›çš„ç‰ˆæœ¬å¹¾ä¹ç›¸åŒï¼Œç‚ºç°¡æ½”çœç•¥) ...
    # ... (å»ºè­°ä¿ç•™æ‚¨åŸæœ¬é«˜æ•ˆçš„çˆ¬å–é‚è¼¯) ...
    video_ids, next_page_token = [], None
    while True:
        pl_request = youtube.playlistItems().list(
            part="contentDetails", playlistId=uploads_playlist_id, maxResults=50, pageToken=next_page_token
        )
        pl_response = pl_request.execute()
        video_ids += [item['contentDetails']['videoId'] for item in pl_response['items']]
        next_page_token = pl_response.get("nextPageToken")
        if not next_page_token or len(video_ids) >= max_videos: break
    
    videos = []
    for i in range(0, min(len(video_ids), max_videos), 50):
        batch = video_ids[i:i+50]
        v_request = youtube.videos().list(part="snippet,statistics", id=",".join(batch))
        v_response = v_request.execute()
        for item in v_response['items']:
            snippet = item['snippet']
            stats = item['statistics']
            videos.append({
                "video_id": item['id'],
                "title": snippet['title'],
                "publishedAt": pd.to_datetime(snippet['publishedAt']),
                "viewCount": int(stats.get('viewCount', 0))
            })
    return pd.DataFrame(videos)


@st.cache_data(ttl=3600)
def get_recent_comments(videos_df, days=180, channel_name=None):
    # ... (æ­¤å‡½å¼å…§å®¹èˆ‡æ‚¨æä¾›çš„ç‰ˆæœ¬ç›¸åŒï¼Œç‚ºç°¡æ½”çœç•¥) ...
    cutoff_date = datetime.now(timezone.utc) - timedelta(days=days)
    videos_df['publishedAt'] = pd.to_datetime(videos_df['publishedAt'])
    recent_videos = videos_df[videos_df['publishedAt'] >= cutoff_date]

    all_comments = []
    progress_bar = st.progress(0, text="æŠ“å–ç•™è¨€ä¸­...")
    for i, vid in enumerate(recent_videos['video_id']):
        try:
            next_page_token = None
            while True:
                c_request = youtube.commentThreads().list(part="snippet", videoId=vid, maxResults=100, pageToken=next_page_token)
                c_response = c_request.execute()
                for item in c_response['items']:
                    comment = item['snippet']['topLevelComment']['snippet']
                    author = comment['authorDisplayName']
                    if channel_name and author == channel_name:
                        continue
                    all_comments.append({
                        "video_id": vid, "author": author, "published_at": comment['publishedAt'],
                        "like_count": comment['likeCount'], "text": comment['textDisplay']
                    })
                next_page_token = c_response.get("nextPageToken")
                if not next_page_token: break
        except Exception as e:
            st.warning(f"æŠ“å–å½±ç‰‡ {vid} ç•™è¨€å¤±æ•—: {e}", icon="âš ï¸")
            continue
        finally:
            progress_bar.progress((i + 1) / len(recent_videos), text=f"æŠ“å–ç•™è¨€ä¸­...({i+1}/{len(recent_videos)})")
    progress_bar.empty()
    return pd.DataFrame(all_comments)

def analyze_channel_with_openai(channel_id, videos_df):
    # ... (æ­¤å‡½å¼å…§å®¹èˆ‡æ‚¨æä¾›çš„ç‰ˆæœ¬ç›¸åŒï¼Œç‚ºç°¡æ½”çœç•¥) ...
    video_text = "\n".join([f"- {row['title']} (è§€çœ‹æ•¸: {row['viewCount']})" for _, row in videos_df.iterrows()])
    prompt = f"""

    ä½ æ˜¯ä¸€ä½é ‚å°–çš„ YouTube é »é“ç­–ç•¥åˆ†æå¸«ã€‚æˆ‘æ­£åœ¨ç ”ç©¶ä¸€å€‹é »é“ï¼Œå…¶ ID ç‚º {channel_id}ã€‚
    è«‹æ ¹æ“šæˆ‘æä¾›çš„æœ€æ–°å½±ç‰‡æ¸…å–®ï¼ˆæ¨™é¡Œèˆ‡ç€è¦½æ•¸ï¼‰ï¼Œç”¨å°ˆæ¥­ã€æœ‰æ¢ç†çš„æ–¹å¼åˆ†æé€™å€‹é »é“ã€‚

    å½±ç‰‡æ¸…å–®:
    {video_text}

    è«‹åš´æ ¼éµå¾ªä»¥ä¸‹ Markdown è¡¨æ ¼æ ¼å¼é€²è¡Œåˆ†æï¼Œä¸è¦æœ‰ä»»ä½•å¤šé¤˜çš„æ–‡å­—æè¿°ï¼š

    ### 1. YouTuberä»‹ç´¹
    | å‰µä½œè€…åç¨± | å°ˆé•· | é¢¨æ ¼ | 
    | :--- | :--- | :--- | 
    | (å‰µä½œè€…åç¨±) | (æ ¹æ“šå½±ç‰‡å…§å®¹æ¨æ¸¬å‰µä½œè€…çš„å°ˆæ¥­é ˜åŸŸ) | (æ ¹æ“šå½±ç‰‡å…§å®¹æ¨æ¸¬å‰µä½œè€…çš„é¢¨æ ¼) |

    ### 2. é »é“ä»‹ç´¹
    | é »é“æ ¸å¿ƒå…§å®¹èˆ‡åƒ¹å€¼ä¸»å¼µ |
    | :--- |
    | (ç¸½çµé »é“çš„æ ¸å¿ƒå…§å®¹èˆ‡åƒ¹å€¼ä¸»å¼µ) |   

    ### 3.1 é »é“å…§å®¹å‰–æ
    | å½±ç‰‡é¡å‹ | ç¯„ä¾‹å½±ç‰‡æ¨™é¡Œ | å½±ç‰‡æ•¸é‡ | å¹³å‡ç€è¦½æ•¸ | ç›®æ¨™å—çœ¾éœ€æ±‚ |
    | :--- | :--- | :--- | :--- | :--- |
    | (ä¾‹å¦‚ï¼šå€‹è‚¡åˆ†æ) | (æŒ‘é¸1-2å€‹ä»£è¡¨æ€§æ¨™é¡Œ) | (å½±ç‰‡æ•¸é‡) | (å¹³å‡ç€è¦½æ•¸) | (èªªæ˜æ»¿è¶³äº†è§€çœ¾ä»€éº¼éœ€æ±‚) |
    | (ä¾‹å¦‚ï¼šå¸‚å ´è¶¨å‹¢) | (æŒ‘é¸1-2å€‹ä»£è¡¨æ€§æ¨™é¡Œ) | (å½±ç‰‡æ•¸é‡) | (å¹³å‡ç€è¦½æ•¸) | (èªªæ˜æ»¿è¶³äº†è§€çœ¾ä»€éº¼éœ€æ±‚) |
    | ... | ... | ... | ... | ... |

    ### 3.2 Top 10 ç†±é–€å½±ç‰‡åˆ†æ
    | æ’å | æ¨™é¡Œåç¨± | ç€è¦½æ•¸ | æ´å¯Ÿåˆ†æ (ç‚ºä½•å—æ­¡è¿) |
    | :--- | :--- | :--- | :--- |
    | 1 | (ç€è¦½æ•¸æœ€é«˜çš„å½±ç‰‡æ¨™é¡Œ) | (å°æ‡‰çš„ç€è¦½æ•¸) | (åˆ†æé€™æ”¯å½±ç‰‡çˆ†ç´…çš„åŸå› ) |
    | 2 | ... | ... | ... |
    | 3 | ... | ... | ... |
    | 4 | ... | ... | ... |
    | 5 | ... | ... | ... |
    | 6 | ... | ... | ... |
    | 7 | ... | ... | ... |
    | 8 | ... | ... | ... |
    | 9 | ... | ... | ... |
    | 10 | ... | ... | ... |

    ### 4. å—çœ¾è¼ªå»“åˆ†æ
    | é‡è¦æ€§æ’åº | å—çœ¾é¡å‹ | å¿ƒç†é©…å‹• | å—çœ¾ç‰¹å¾µ | è§€çœ‹è¡Œç‚º/å…§å®¹åå¥½ | ä»£è¡¨å½±ç‰‡ï¼ˆè§€çœ‹æ•¸ï¼‰
    | :--- | :--- | :--- | :--- | :--- | :--- |
    | (æ ¹æ“šè©²å½±ç‰‡é¡å‹å½±ç‰‡æ•¸ä»¥åŠå¹³å‡ç€è¦½æ•¸å…©å€‹ç¶­åº¦åˆ†æï¼Œå°‡è©²å½±ç‰‡é¡å‹çš„å—çœ¾ä¾ç…§é‡è¦æ€§æ’åº) | (è©²å½±ç‰‡é¡å‹çš„å—çœ¾é¡å‹) | (è©²å½±ç‰‡é¡å‹å—çœ¾çš„å¿ƒç†é©…å‹•) | (è©²å½±ç‰‡é¡å‹çš„å—çœ¾ç‰¹å¾µ) | (è©²å½±ç‰‡é¡å‹å—çœ¾è§€çœ‹è¡Œç‚º/å…§å®¹åå¥½) | (è©²å½±ç‰‡é¡å‹ä»£è¡¨å½±ç‰‡èˆ‡è§€çœ‹æ•¸) |
    | (å¯è‡ªè¡Œå¢åŠ ) | ... | ... | ... | ... | ... |

    """
    response = client.chat.completions.create(
        model="gpt-5",
        messages=[{"role":"user","content": prompt}]
    )
    return response.choices[0].message.content

def analyze_comments_with_openai(channel_id, comments_df):
    # ... (æ­¤å‡½å¼å…§å®¹èˆ‡æ‚¨æä¾›çš„ç‰ˆæœ¬ç›¸åŒï¼Œç‚ºç°¡æ½”çœç•¥) ...
    comment_text = "\n".join([f"- {text}" for text in comments_df['text'].tolist()])
    prompt = f"""
    ä½ æ˜¯ä¸€ä½æ•éŠ³çš„å¸‚å ´åˆ†æèˆ‡ç”¢å“é–‹ç™¼å°ˆå®¶ã€‚æˆ‘æ­£åœ¨ç ”ç©¶ ID ç‚º {channel_id} çš„ YouTube é »é“ï¼Œä¸¦æ”¶é›†äº†è§€çœ¾æœ€è¿‘çš„æå•ç•™è¨€ã€‚
    è«‹æ ¹æ“šé€™äº›ç•™è¨€ï¼Œåˆ†æç²‰çµ²çš„ç—›é»ï¼Œä¸¦æå‡ºå…·é«”çš„è®Šç¾å»ºè­°ï¼ˆä¾‹å¦‚ï¼šç·šä¸Šèª²ç¨‹æˆ– Appï¼‰ã€‚

    ç”¨æˆ¶æå•ç•™è¨€:
    {comment_text}

    è«‹åš´æ ¼éµå¾ªä»¥ä¸‹ Markdown è¡¨æ ¼æ ¼å¼é€²è¡Œåˆ†æï¼Œä¸è¦æœ‰ä»»ä½•å¤šé¤˜çš„æ–‡å­—æè¿°ï¼š

    ### 1. ç²‰çµ²ç—›é»åˆ†æ
    | ç—›é»åˆ†é¡ | æ ¸å¿ƒå•é¡Œ | ç•™è¨€æ•¸ | ç•™è¨€ç¯„ä¾‹ |
    | :--- | :--- | :--- | :--- |
    | **(ä¾‹å¦‚ï¼šçŸ¥è­˜ç³»çµ±åŒ–)** | ç²‰çµ²è¦ºå¾—è³‡è¨Šé›¶æ•£ï¼Œå¸Œæœ›èƒ½æœ‰ç³»çµ±åœ°å­¸ç¿’ã€‚ | (ä¼°ç®—è©²ç—›é»é¡å‹ç•™è¨€æ•¸) | (æŒ‘é¸1-2å‰‡ä»£è¡¨æ€§ç•™è¨€) |
    | **(ä¾‹å¦‚ï¼šå¯¦ä½œå›°é›£)** | çŸ¥é“ç†è«–ä½†ä¸çŸ¥å¦‚ä½•å¯¦éš›æ“ä½œæˆ–æ‡‰ç”¨ã€‚ | (ä¼°ç®—è©²ç—›é»é¡å‹ç•™è¨€æ•¸) | (æŒ‘é¸1-2å‰‡ä»£è¡¨æ€§ç•™è¨€) |
    | **(å¯è‡ªè¡Œå¢åŠ )** | ... | ... |

    ### 2. å•†æ¥­è®Šç¾å»ºè­°
    | æ¬²è§£æ±ºçš„ç—›é» | è§£æ±ºæ–¹æ¡ˆ | ç†ç”± | æ¨è–¦å…§å®¹/åŠŸèƒ½ |
    | :--- | :--- | :--- | :--- |
    | (æƒ³è¦è§£æ±ºçš„ç²‰çµ²ç—›é»åˆ†é¡ï¼Œæ ¹æ“šä¸Šæ–¹1. ç²‰çµ²ç—›é»åˆ†æçš„ç—›é»åˆ†é¡) | (è§£æ±ºæ–¹æ¡ˆå»ºè­°) | (èªªæ˜ç‚ºä½•é€™å€‹æ–¹æ¡ˆé©åˆè§£æ±ºç²‰çµ²ç—›é») | (å…·é«”æå‡ºèª²ç¨‹å–®å…ƒæˆ– App æ ¸å¿ƒåŠŸèƒ½) |
    """
    response = client.chat.completions.create(
        model="gpt-5",
        messages=[{"role":"user","content": prompt}]
    )
    return response.choices[0].message.content

def plot_top_questions_plotly(comments_df, top_n=10):
    # ... (æ­¤å‡½å¼å…§å®¹èˆ‡æ‚¨æä¾›çš„ç‰ˆæœ¬ç›¸åŒ) ...
    pass # ç‚ºç°¡æ½”çœç•¥

# ========= Streamlit UI (å…¨æ–°æ”¹å¯«) =========
# <<< ä¿®æ”¹é» 2: å…¨æ–°çš„ UI æµç¨‹ >>>
st.title("ğŸ“Š YouTube é »é“åˆ†æå·¥å…·")

# --- éšæ®µ 1: è¼¸å…¥èˆ‡æ¨¡å¼é¸æ“‡ ---
st.header("Step 1: è¼¸å…¥ç›®æ¨™é »é“èˆ‡é¸æ“‡æ¨¡å¼")
channel_id_input = st.text_input("è¼¸å…¥ YouTube Channel ID", placeholder="ä¾‹å¦‚ï¼šUC-qgS_2Q2nF_3a9hAlh_aYg")
analysis_option = st.radio(
    "é¸æ“‡åˆ†ææ¨¡å¼",
    ("å…¨é »é“åˆ†æ (åˆ†æå½±ç‰‡é¡å‹èˆ‡å—çœ¾)", "ç•™è¨€ç—›é»åˆ†æ (åˆ†æç²‰çµ²å•é¡Œèˆ‡éœ€æ±‚)"),
    horizontal=True, key="analysis_option"
)

# --- éšæ®µ 2: æ ¹æ“šæ¨¡å¼é¡¯ç¤ºå°æ‡‰åŠŸèƒ½ ---
if analysis_option == "å…¨é »é“åˆ†æ (åˆ†æå½±ç‰‡é¡å‹èˆ‡å—çœ¾)":
    st.header("Step 2: å…¨é »é“åˆ†æ")
    if st.button("æŠ“å–é »é“æ‰€æœ‰å½±ç‰‡", key="fetch_videos"):
        if channel_id_input:
            with st.spinner("æŠ“å–é »é“è³‡è¨Šä¸­..."):
                uploads_id, _ = get_channel_info(channel_id_input)
                if not uploads_id:
                    st.error("æ‰¾ä¸åˆ°è©²é »é“ï¼Œè«‹æª¢æŸ¥ Channel ID æ˜¯å¦æ­£ç¢ºã€‚")
                else:
                    st.session_state.videos_df = get_channel_videos(uploads_id)
                    st.success(f"æˆåŠŸæŠ“å– {len(st.session_state.videos_df)} æ”¯å½±ç‰‡ï¼")
        else:
            st.warning("è«‹å…ˆè¼¸å…¥ Channel ID")

    if 'videos_df' in st.session_state:
        st.subheader("å½±ç‰‡æ¸…å–®é è¦½")
        st.dataframe(st.session_state.videos_df.head(10))
        st.download_button("ä¸‹è¼‰å®Œæ•´å½±ç‰‡æ¸…å–® (CSV)", st.session_state.videos_df.to_csv(index=False).encode("utf-8-sig"), "videos.csv")
        
        # <<< ä¿®æ”¹é» 3: ç¨ç«‹çš„ AI åˆ†ææŒ‰éˆ• >>>
        if st.button("ğŸš€ ä½¿ç”¨ OpenAI é€²è¡Œæ·±åº¦åˆ†æ", key="openai_channel_analysis"):
            with st.spinner("ğŸ¤– OpenAI æ­£åœ¨åˆ†æé »é“ä¸­ï¼Œè«‹ç¨å€™..."):
                analysis_result = analyze_channel_with_openai(channel_id_input, st.session_state.videos_df)
                st.session_state.channel_analysis_result = analysis_result
    
    if 'channel_analysis_result' in st.session_state:
        st.subheader("ğŸ¤– OpenAI å…¨é »é“åˆ†æçµæœ")
        # <<< ä¿®æ”¹é» 4: ä½¿ç”¨ st.markdown é¡¯ç¤ºæ¼‚äº®çš„è¡¨æ ¼ >>>
        st.markdown(st.session_state.channel_analysis_result)

elif analysis_option == "ç•™è¨€ç—›é»åˆ†æ (åˆ†æç²‰çµ²å•é¡Œèˆ‡éœ€æ±‚)":
    st.header("Step 2: ç•™è¨€ç—›é»åˆ†æ")
    days = st.number_input("è¨­å®šè¦åˆ†ææœ€è¿‘å¹¾å¤©å…§çš„å½±ç‰‡ç•™è¨€", min_value=7, max_value=3650, value=180, step=1)
    
    if st.button("æŠ“å–è¿‘æœŸç•™è¨€", key="fetch_comments"):
        if channel_id_input:
            with st.spinner("æŠ“å–é »é“èˆ‡å½±ç‰‡è³‡è¨Šä¸­..."):
                uploads_id, channel_title = get_channel_info(channel_id_input)
                if not uploads_id:
                    st.error("æ‰¾ä¸åˆ°è©²é »é“ï¼Œè«‹æª¢æŸ¥ Channel ID æ˜¯å¦æ­£ç¢ºã€‚")
                else:
                    videos_df = get_channel_videos(uploads_id, max_videos=200) # ç•™è¨€åˆ†æä¸éœ€è¦å…¨æŠ“ï¼ŒæŠ“è¿‘æœŸå½±ç‰‡å³å¯
                    st.session_state.comments_df = get_recent_comments(videos_df, days=days, channel_name=channel_title)
                    st.success(f"æˆåŠŸæŠ“å– {len(st.session_state.comments_df)} å‰‡ç•™è¨€ï¼")
        else:
            st.warning("è«‹å…ˆè¼¸å…¥ Channel ID")

    if 'comments_df' in st.session_state:
        st.subheader(f"æœ€è¿‘ {days} å¤©ç•™è¨€é è¦½")
        st.dataframe(st.session_state.comments_df.head(10))
        st.download_button("ä¸‹è¼‰å®Œæ•´ç•™è¨€æ¸…å–® (CSV)", st.session_state.comments_df.to_csv(index=False).encode("utf-8-sig"), "comments.csv")

        # <<< ä¿®æ”¹é» 3: ç¨ç«‹çš„ AI åˆ†ææŒ‰éˆ• >>>
        if st.button("ğŸš€ ä½¿ç”¨ OpenAI åˆ†æç²‰çµ²ç—›é»", key="openai_comment_analysis"):
            with st.spinner("ğŸ¤– OpenAI æ­£åœ¨åˆ†æç•™è¨€ä¸­ï¼Œè«‹ç¨å€™..."):
                # ç°¡å–®ç¯©é¸åŒ…å«å•å¥çš„ç•™è¨€çµ¦ AIï¼Œæé«˜åˆ†æç²¾æº–åº¦
                question_patterns = r"\?|ï¼Ÿ|æ€éº¼|å¦‚ä½•|ç‚ºä»€éº¼|å—|èƒ½ä¸èƒ½|å¯ä¸å¯ä»¥|æ€ä¹ˆ|ä¸ºä»€ä¹ˆ|å—"
        
        
                questions_df = st.session_state.comments_df[st.session_state.comments_df['text'].str.contains(question_patterns, na=False, regex=True)]
                if questions_df.empty:
                    st.warning("æ‰¾ä¸åˆ°åŒ…å«å•é¡Œçš„ç•™è¨€ï¼Œç„¡æ³•é€²è¡Œç—›é»åˆ†æã€‚")
                    st.session_state.comment_analysis_result = "æ‰¾ä¸åˆ°å¯åˆ†æçš„å•é¡Œç•™è¨€ã€‚"
                else:
                    analysis_result = analyze_comments_with_openai(channel_id_input, questions_df)
                    st.session_state.comment_analysis_result = analysis_result

    if 'comment_analysis_result' in st.session_state:
        st.subheader("ğŸ¤– OpenAI ç²‰çµ²ç—›é»åˆ†æçµæœ")
        # <<< ä¿®æ”¹é» 4: ä½¿ç”¨ st.markdown é¡¯ç¤ºæ¼‚äº®çš„è¡¨æ ¼ >>>
        st.markdown(st.session_state.comment_analysis_result)